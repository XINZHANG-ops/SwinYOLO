{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "protective-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from object_detect.convert_detection_image_to_classification_image import obj_to_cls\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import cv2\n",
    "import os\n",
    "from object_detect.convert_xml_label_to_txt import convert\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "received-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "from YoloLoss import *\n",
    "from ObjectDetectUtils import *\n",
    "from SwinYOLOClass import *\n",
    "from DataProcess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-first",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-income",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from object_detect.convert_xml_label_to_txt import convert\n",
    "# import os\n",
    "# import shutil\n",
    "# xml_dir = 'linmao-new'\n",
    "# out_dir = f'{xml_dir}_txt'\n",
    "# label_map = {'nl_0438': 0,\n",
    "#              'nl_0431': 1,\n",
    "#              'nl_0239': 2,\n",
    "#              'nl_0238': 3,\n",
    "#              'nl_0271': 4,\n",
    "#              'nl_0280': 5,\n",
    "#              'nl_0433': 6,\n",
    "#              'nl_0224': 7,\n",
    "#              'nl_0098': 8}\n",
    "# label_dict = convert(xml_dir, out_dir, label_map, True)\n",
    "# # see what files we kept\n",
    "# label_path = []\n",
    "\n",
    "# for root, dirs, files in os.walk(out_dir):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\".txt\"):\n",
    "#             the_path = os.path.join(root, file)\n",
    "#             label_path.append(the_path)\n",
    "# all_label_names = set([os.path.splitext(i.split(os.sep)[-1])[0] for i in label_path])\n",
    "# # only keep these images with labels\n",
    "# image_path = []\n",
    "\n",
    "# for root, dirs, files in os.walk(xml_dir):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\".jpg\"):\n",
    "#             the_path = os.path.join(root, file)\n",
    "#             file_name = os.path.splitext(the_path.split(os.sep)[-1])[0]\n",
    "#             if file_name in all_label_names:\n",
    "#                 image_path.append(the_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(os.path.join(out_dir, image_path[0].split(os.sep)[1:-1][0]), exist_ok=True)\n",
    "# for img_path in image_path:\n",
    "#     file_name_img = img_path.split('/')[-1]\n",
    "#     shutil.copy2(img_path, os.path.join(out_dir, image_path[0].split(os.sep)[1:-1][0], file_name_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_dir = 'linmao-new_txt/JPEGImages'\n",
    "# image_names = []\n",
    "# label_names = []\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# for root, dirs, files in os.walk(images_dir):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\".jpg\"):\n",
    "#             if 'checkpoint' in file:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 name = os.path.splitext(file_path.split(os.sep)[-1])[0]\n",
    "#                 image_names.append(name+'.jpg')\n",
    "#                 label_names.append(name+'.txt')\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     image_names, label_names, test_size=0.2, random_state=42)\n",
    "\n",
    "# train_csv = pd.DataFrame(list(zip(X_train, y_train)))\n",
    "# test_csv = pd.DataFrame(list(zip(X_test, y_test)))\n",
    "# train_csv.to_csv('train.csv', index=False, header = False)\n",
    "# test_csv.to_csv('test.csv', index=False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-orange",
   "metadata": {},
   "source": [
    "# resize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# dsize = (320, 320)\n",
    "\n",
    "# images_dir = 'linmao-new_txt/JPEGImages'\n",
    "\n",
    "# save_dir = 'linmao-new_txt/resized/'\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# image_path = []\n",
    "# import pandas as pd\n",
    "# for root, dirs, files in os.walk(images_dir):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\".jpg\"):\n",
    "#             if 'checkpoint' in file:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 image_path.append(file_path)\n",
    "# for img_p in tqdm(image_path):\n",
    "#     name = img_p.split(os.sep)[-1]\n",
    "#     src = cv2.imread(img_p)\n",
    "#     frame = cv2.resize(src,dsize)\n",
    "#     cv2.imwrite(os.path.join(save_dir, name),frame) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from object_detect.box_image_text import plot_box\n",
    "# plot_box('linmao-new_txt/resized', 'linmao-new_txt/Annotations', 'boxed', thickness=3, filter_diff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main file for training Yolo model on Pascal VOC dataset\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as FT\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters etc. \n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "BATCH_SIZE = 16 # 64 in original paper but I don't have that much vram, grad accum?\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 1000\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"overfit.pth.tar\"\n",
    "IMG_DIR = \"linmao-new_txt/resized\"\n",
    "LABEL_DIR = \"linmao-new_txt/Annotations\"\n",
    "num_classes = 9\n",
    "img_size = 320\n",
    "split_size = 14\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((img_size, img_size)), transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-avatar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-adrian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-apache",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that img_size/window_size should be 32\n",
    "model = SwinTransformerYOLOv1(img_size=img_size, patch_size=4, embed_dim=96, window_size=int(img_size/32), split_size = split_size, \n",
    "                              num_boxes = 2, num_classes=num_classes).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SwinTransformerYOLOv1(img_size=img_size, split_size = split_size, \n",
    "#                  num_boxes = 2, patch_size=4, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "loss_fn = YoloLoss(S=split_size, B=2, C=num_classes, obj_w=2, noobj_w=2, coord_w=1, cls_w=5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xin_util.PrettyPrintDict import pretty_print_dict\n",
    "\n",
    "train_dataset = VOCDataset(\n",
    "    \"train.csv\",\n",
    "    transform=transform,\n",
    "    img_dir=IMG_DIR,\n",
    "    label_dir=LABEL_DIR,\n",
    "    S=split_size, B=2, C=num_classes\n",
    ")\n",
    "\n",
    "test_dataset = VOCDataset(\n",
    "    \"test.csv\", transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR,\n",
    "    S=split_size, B=2, C=num_classes\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "save_step = 10\n",
    "\n",
    "step = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    train_csv = pd.read_csv('train.csv', header=None)\n",
    "    sample_df = train_csv.sample(n=3)\n",
    "    sample_df.to_csv('sample.csv', index=False, header = False)\n",
    "\n",
    "    sample_dataset = VOCDataset(\n",
    "        \"sample.csv\",\n",
    "        transform=transform,\n",
    "        img_dir=IMG_DIR,\n",
    "        label_dir=LABEL_DIR,\n",
    "        S=split_size, B=2, C=num_classes\n",
    "    )\n",
    "\n",
    "    sample_loader = DataLoader(\n",
    "        dataset=sample_dataset,\n",
    "        batch_size=2,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    for x, y in sample_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        for idx in range(1):\n",
    "            bboxes = cellboxes_to_boxes(model(x), S=split_size, C=num_classes)\n",
    "            bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "            plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "    \n",
    "    \n",
    "#     for x, y in train_loader:\n",
    "#         x = x.to(DEVICE)\n",
    "#         for idx in range(8):\n",
    "#             bboxes = cellboxes_to_boxes(model(x))\n",
    "#             bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "#             plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "\n",
    "#     pred_boxes, target_boxes = get_bboxes(\n",
    "#         train_loader, model, iou_threshold=0.5, threshold=0.4, S=split_size, C=num_classes\n",
    "#     )\n",
    "    \n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        test_loader, model, iou_threshold=0.5, threshold=0.4, S=split_size, C=num_classes\n",
    "    )\n",
    "\n",
    "    label_wise_precision, label_wise_recall, label_wise_ap, label_wise_pr_curve = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=num_classes\n",
    "    )\n",
    "    print(f\"Test mAP:\")\n",
    "    pretty_print_dict(label_wise_ap)\n",
    "    step+=1\n",
    "    if step == save_step:\n",
    "        checkpoint = {\n",
    "           \"state_dict\": model.state_dict(),\n",
    "           \"optimizer\": optimizer.state_dict()}\n",
    "        save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
    "        step = 0\n",
    "\n",
    "#     if mean_avg_prec > 0.9:\n",
    "#        checkpoint = {\n",
    "#            \"state_dict\": model.state_dict(),\n",
    "#            \"optimizer\": optimizer.state_dict(),\n",
    "#        }\n",
    "#        save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
    "#        import time\n",
    "#        time.sleep(10)\n",
    "\n",
    "    train_fn(train_loader, model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-swiss",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-bobby",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-phone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-factor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-religious",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-gateway",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dsize = (384, 384)\n",
    "\n",
    "images_dir = 'linmao-new_txt/JPEGImages'\n",
    "\n",
    "save_pred_dir = 'swinyolo_pred'\n",
    "os.makedirs(save_pred_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "class ComposeImg(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "transform_img = ComposeImg([transforms.Resize((img_size, img_size)), transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "\n",
    "image_path = []\n",
    "import pandas as pd\n",
    "for root, dirs, files in os.walk(images_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            if 'checkpoint' in file:\n",
    "                pass\n",
    "            else:\n",
    "                file_path = os.path.join(root, file)\n",
    "                image_path.append(file_path)\n",
    "                \n",
    "for img_p in tqdm(image_path):\n",
    "    name = os.path.splitext(img_p.split(os.sep)[-1])[0]\n",
    "    \n",
    "    image = Image.open(img_p)\n",
    "    img = transform_img(image)\n",
    "    img = torch.reshape(img, (1, *img.shape))\n",
    "\n",
    "    bboxes = cellboxes_to_boxes(model(img), S=split_size, C=num_classes)\n",
    "    bboxes = [non_max_suppression(box, iou_threshold=0.5, threshold=0.3, box_format=\"midpoint\") for box in bboxes]\n",
    "    bboxes = bboxes[0] # since we only input one image per run, if it is a bacth we can change this\n",
    "    save_path = os.path.join(save_pred_dir, name+'.txt')\n",
    "    with open(save_path, \"w+\") as f:\n",
    "        result = []\n",
    "        for box in bboxes:\n",
    "            label, conf, x, y, w, h = int(box[0]), box[1], box[2], box[3], box[4], box[5]\n",
    "            result.append(f\"{label} {x} {y} {w} {h} {conf}\")\n",
    "        f.write(\"\\n\".join(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 'nl_0438', \n",
    "             1: 'nl_0431', \n",
    "             2: 'nl_0239', \n",
    "             3: 'nl_0238', \n",
    "             4: 'nl_0271', \n",
    "             5: 'nl_0280', \n",
    "             6: 'nl_0433', \n",
    "             7: 'nl_0224', \n",
    "             8: 'nl_0098'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detect.metrics import label_metrics\n",
    "pred_txt_dir = 'swinyolo_pred' # runs/detect/combined_yolo_cls/labels\n",
    "truth_txt_dir = 'linmao-new_txt/Annotations'\n",
    "lm = label_metrics(pred_txt_dir, truth_txt_dir)\n",
    "summary_df = lm.get_metrics(iou_thresholds=[0.5], interpolate_pr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.label = summary_df.label.replace(label_map)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(131)\n",
    "x = [str(i) for i in summary_df['label']]\n",
    "precisions = summary_df['P@0.5'].tolist()\n",
    "plt.title('precision')\n",
    "plt.bar(x, precisions)\n",
    "plt.xticks(rotation=90)\n",
    "for i, v in enumerate(x):\n",
    "    plt.text(i-0.4, precisions[i]+0.01, str(round(precisions[i], 2)), color='blue', fontweight='bold')\n",
    "\n",
    "plt.subplot(132)\n",
    "recalls = summary_df['R@0.5'].tolist()\n",
    "plt.title('recall')\n",
    "plt.bar(x, recalls)\n",
    "plt.xticks(rotation=90)\n",
    "for i, v in enumerate(x):\n",
    "    plt.text(i-0.4, recalls[i]+0.01, str(round(recalls[i], 2)), color='blue', fontweight='bold')\n",
    "\n",
    "plt.subplot(133)\n",
    "aps = summary_df['AP@0.5'].tolist()\n",
    "plt.title('average precision')\n",
    "plt.bar(x, aps)\n",
    "plt.xticks(rotation=90)\n",
    "for i, v in enumerate(x):\n",
    "    plt.text(i-0.4, aps[i]+0.01, str(round(aps[i], 2)), color='blue', fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detect.box_image_text import plot_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box(images_dir, 'swinyolo_pred', 'boxed_pred', thickness=3, filter_diff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-street",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-sucking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "rapids-gpu.0-18.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/rapids-gpu.0-18:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
